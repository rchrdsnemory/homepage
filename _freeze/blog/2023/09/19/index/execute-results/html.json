{
  "hash": "47648560ec11ecc2b0de541598f455aa",
  "result": {
    "markdown": "---\ntitle: 'Five methods of categorical encoding: one-hot, label, frequency, target, and embedding'\ndate: 2023-09-19\ndescription: \"In this post I take a recent playground dataset from kaggle that contains categorical variables and apply five methods of categorical encoding: one-hot, label, frequency, target, and embedding. Then I fit logistic, random forest, and xgboost models using each of the five encoding methods and evaluate the impact on model performance.\"\nimage: social-image.png\ntwitter-card:\n  image: \"social-image.png\"\nopen-graph:\n  image: \"social-image.png\"\ncategories:\n  - kaggle\n  - categorical-encoding\n  - machine-learning\nfreeze: true\n---\n\n\nIn this post I'll be using the dataset from [Kaggle's Playground Series S3E22: Predict Health Outcomes of Horses](https://www.kaggle.com/competitions/playground-series-s3e22/data) to examine 5 categorical encoding methods. \n\nHere's a list of the encoding methods that I'll be using:\n- One-hot encoding\n- Label encoding\n- Frequency encoding\n- Target encoding\n- Embedding encoding\n\nFirst I'll walk through and demonstrate each of the encoding methods, then I'll create recipes and workflows for tidymodels. Then I'll fit each of the 5 encodings using logistic, random forest, and xgboost models, and finally I'll evaluate and compare the performance of the 3 models with each of the 5 encodings.\n\n# Load libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(lares)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(h2o)\nlibrary(encodeR)\n\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         2 days 2 hours \n    H2O cluster timezone:       America/New_York \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.42.0.2 \n    H2O cluster version age:    1 month and 25 days \n    H2O cluster name:           H2O_started_from_R_tyler_wkj161 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.12 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.1 (2023-06-16) \n```\n:::\n\n```{.r .cell-code}\nh2o.no_progress()\n```\n:::\n\n\n# Load the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_csv('data/playground-series-s3e22/train.csv', show_col_types = F)\n```\n:::\n\n\n# Categorical variables\n\nThis dataset contains 17 categorical (character) columns. It's also worth noting that there are no missing values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% lares::df_str(return = \"plot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in .font_global(font, quiet = FALSE): Font 'Arial Narrow' is not\ninstalled, has other name, or can't be found\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nLooking at the character columns, I see that some of the variables have as many as 7 levels. These aren't exactly high cardinality variables, but it'll be enough to experiment with the encoding methods. I also noticed some case inconsistency (i.e., some columns have both \"None\" and \"none\"). Converting all strings to lowercase would help to at least combine the \"none\" types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchar_cols <- colnames(df %>% select(where(is.character)))\nfor(col in char_cols){\n  print(paste0(toupper(col), ': ', paste0(distinct(df[col])[[1]], collapse=', ')))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SURGERY: yes, no\"\n[1] \"AGE: adult, young\"\n[1] \"TEMP_OF_EXTREMITIES: cool, cold, normal, warm, None\"\n[1] \"PERIPHERAL_PULSE: reduced, normal, None, absent, increased\"\n[1] \"MUCOUS_MEMBRANE: dark_cyanotic, pale_cyanotic, pale_pink, normal_pink, bright_pink, bright_red, None\"\n[1] \"CAPILLARY_REFILL_TIME: more_3_sec, less_3_sec, None, 3\"\n[1] \"PAIN: depressed, mild_pain, extreme_pain, alert, severe_pain, None, slight\"\n[1] \"PERISTALSIS: absent, hypomotile, normal, hypermotile, None, distend_small\"\n[1] \"ABDOMINAL_DISTENTION: slight, moderate, none, severe, None\"\n[1] \"NASOGASTRIC_TUBE: slight, none, significant, None\"\n[1] \"NASOGASTRIC_REFLUX: less_1_liter, more_1_liter, none, None, slight\"\n[1] \"RECTAL_EXAM_FECES: decreased, absent, None, normal, increased, serosanguious\"\n[1] \"ABDOMEN: distend_small, distend_large, normal, firm, None, other\"\n[1] \"ABDOMO_APPEARANCE: serosanguious, cloudy, clear, None\"\n[1] \"SURGICAL_LESION: yes, no\"\n[1] \"CP_DATA: no, yes\"\n[1] \"OUTCOME: died, euthanized, lived\"\n```\n:::\n:::\n\n\n# Train/test split\n\nBefore diving into categorical encoding methods, I'll do a train/test split. I'l also convert the character columns to lowercase to address the problem I mentioned above (\"None\" vs. \"none\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\ndf %>% mutate_if(where(is.character), .funs=tolower) %>%\n  mutate(outcome = as.factor(outcome)) %>%\n  mutate(across(where(is.character), factor)) -> df\n\nsplit <- initial_split(df)\ntrain <- training(split)\ntest <- testing(split)\n```\n:::\n\n\n# Categorical encoding\n\n## One-hot encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_1hot_with_novel <- \n  recipe(outcome ~ ., data = train %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_novel(all_nominal_predictors(), new_level = \"NA\") %>%\n  step_dummy(all_nominal_predictors(), one_hot=T)\n```\n:::\n\n\nThe first -- and probably most popular -- type of categorical encoding is one-hot encoding. One-hot encoding transforms a single categorical variable with N levels into binary variables encoding each of the N levels.\n\nFor example, `age` is a categorical variable with 2 levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(train$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"adult\" \"young\"\n```\n:::\n\n```{.r .cell-code}\nlength(levels(train$age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nWhen `age` is one-hot encoded, a column is created for each level to encode the value (e.g., if the original value was `adult`, then the `age_adult` column gets a 1 and the other columns get a 0). And since I've also included a step to encode novel levels as `NA`, there is also a third column for that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_1hot_with_novel %>%\n  prep() %>%\n  bake(new_data = NULL) %>%\n  select(starts_with('age')) %>%\n  head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  age_adult age_young age_NA.\n      <dbl>     <dbl>   <dbl>\n1         1         0       0\n2         1         0       0\n3         1         0       0\n```\n:::\n:::\n\n## Label encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_label <- \n  recipe(outcome ~ ., data = train %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_integer(all_nominal_predictors())\n```\n:::\n\n\nWith label encoding, each level of the categorical variable is given an (arbitrary) number. In the `tidymodels` framework, `step_integer` works like scikit's `LabelEncoder`, and encodes new values as zero. Here we see that one level of `age` was encoded as \"1\" and the other was encoded as \"2\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_label %>%\n  prep() %>%\n  bake(new_data = NULL) %>%\n  select(age) %>%\n  distinct\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 1\n    age\n  <int>\n1     1\n2     2\n```\n:::\n:::\n\n\n## Frequency encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_encoding <- encodeR::frequency_encoder(\n  X_train = train,\n  X_test = test, \n  cat_columns = colnames(df %>% select(where(is.factor), -outcome))\n)\n\ntrain_freq <- freq_encoding$train\ntest_freq <- freq_encoding$test\n```\n:::\n\n\nWith frequency encoding, levels of the categorical variable are replaced with their frequency. Here, we can see how the levels of `age` have been replaced with their frequency in the training set. (When this is applied to the test set, these same training frequencies will be used.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_freq %>%\n  select(age) %>%\n  distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 1\n     age\n   <dbl>\n1 0.937 \n2 0.0626\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_freq <- \n  recipe(outcome ~ ., data = train_freq %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n## Target encoding\n\nFor target encoding (also called \"effect encoding\" or \"likelihood encoding\"), I'll be using the `h2o` package because it supports multi-class targets. (The `embed` package can also do target encoding and integrates better with a tidymodels workflow, but at the moment it only supports binary targets.)\n\nUsing `h2o` requires some additional setup.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to h2o format\ndf_h2o <- as.h2o(df)\n\n# Split the dataset into train and test\nsplits_h2o <- h2o.splitFrame(data = df_h2o, ratios = .8, seed = 42)\ntrain_h2o <- splits_h2o[[1]]\ntest_h2o <- splits_h2o[[2]]\n```\n:::\n\n\nWith target encoding, the levels of the categorical variable are replaced with their mean value on the target. For example, if the level \"young\" was associated with a mean target value of 0.75, then this is the value with which that level would be replaced. \n\nBecause the outcome is being used for encoding, care needs to be taken when using this method to avoid leakage and overfitting. In this case, I'll use the \"Leave One Out\" method: for each row, the mean is calculated over all rows excluding that row.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose which columns to encode\nencode_columns <- colnames(df %>% select(where(is.factor), -outcome)) # All categorical variables\n\n# Train a TE model\nte_model <- h2o.targetencoder(x = encode_columns,\n                              y = 'outcome', \n                              keep_original_categorical_columns=T,\n                              training_frame = train_h2o,\n                              noise=0,\n                              seed=100,\n                              blending = T, # Blending helps with levels that are more rare\n                              data_leakage_handling = \"LeaveOneOut\")\n\n# New target encoded training and test datasets\ntrain_te <- h2o.transform(te_model, train_h2o)\ntest_te <- h2o.transform(te_model, test_h2o)\n```\n:::\n\n\nHere we can see how the target encoding strategy encoded `age`: Two new variables are created, `age_euthanized_te` and `age_lived_te`. The encoded values represent the proportion of cases that were euthanized, or lived, for each level of `age`. (Note: The \"died\" level of the outcome variable is missing. This is because if we know the proportion that were euthanized and lived, we also know the proportion that died.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_te %>%\n  as.data.frame() %>%\n  select(starts_with('age') & ends_with('te'), age) %>%\n  distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age_euthanized_te age_lived_te   age\n1        0.20937841    0.4776445 adult\n2        0.06005923    0.2157985 young\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop the unencoded columns\ntrain_te %>% \n  as.data.frame() %>%\n  select(-all_of(encode_columns)) %>%\n  as.h2o() -> train_te\ntest_te %>% \n  as.data.frame() %>%\n  select(-all_of(encode_columns)) %>%\n  as.h2o() -> test_te\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a recipe to use later\nrecipe_target <- \n  recipe(outcome ~ ., data = train_te %>% as.data.frame() %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n# Embedding encoding\n\nEmbedding encoding is a new technique inspired by deep learning that converts categorical variables into numeric vectors. It uses deep learning to learn the vector representation, and it can be used to do dimensionality reduction on categorical variables.\n\nFor example, the variable `pain` has 7 levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(train$pain)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"alert\"        \"depressed\"    \"extreme_pain\" \"mild_pain\"    \"none\"        \n[6] \"severe_pain\"  \"slight\"      \n```\n:::\n:::\n\n\nBut using embeddings, I can \"project\" these 7 levels onto a smaller set of dimensions -- say 3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npain_embedding <- \n  recipe(outcome ~ ., data = train %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  embed::step_embed(pain, \n                    outcome = vars(outcome),\n                    predictors = all_numeric_predictors(),\n                    hidden_units = 2,\n                    num_terms = 3,\n                    keep_original_cols = T)\n\ntensorflow::set_random_seed(42)\npain_embedding %>%\n  prep() %>%\n  bake(new_data = NULL) %>%\n  select(starts_with('pain')) %>%\n  distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  pain         pain_embed_1 pain_embed_2 pain_embed_3\n  <fct>               <dbl>        <dbl>        <dbl>\n1 mild_pain         0.0466      -0.00297     0.000295\n2 depressed         0.0470      -0.0332      0.00238 \n3 severe_pain      -0.00442     -0.0437     -0.0509  \n4 alert             0.0450      -0.0422      0.0152  \n5 extreme_pain      0.0131      -0.0414     -0.00536 \n6 none              0.0293      -0.0480     -0.0285  \n```\n:::\n:::\n\n\nSince embeddings are useful for dimensionality reduction, this means it's going to be most useful for categorical variables with more levels. So I don't want to apply embeddings to variables with few levels to begin with: I'll one-hot encode all variables with 3 or fewer levels, and then apply embeddings to variables with more than 3, and project them down to only 3 levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchar_cols <- colnames(train %>% select(where(is.factor), -outcome))\ncols_for_onehot <- c()\ncols_for_embdding <- c()\nfor(col in char_cols){\n  if(nrow(distinct(train[col])) <= 3){\n    cols_for_onehot = append(cols_for_onehot, col)\n  }\n  else {\n    cols_for_embdding = append(cols_for_embdding, col)\n  }\n}\n\nrecipe_embedding <- \n  recipe(outcome ~ ., data = train %>% select(-id)) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_novel(all_of(cols_for_onehot), new_level = \"NA\") %>%\n  step_dummy(all_of(cols_for_onehot), one_hot=T) %>%\n  embed::step_embed(all_of(cols_for_embdding), \n                    outcome = vars(outcome),\n                    predictors = all_numeric_predictors(),\n                    hidden_units = 2,\n                    num_terms = 3,\n                    keep_original_cols = F)\n```\n:::\n\n\n# Modeling\n\nNow onto some modeling.\n\nI'll define 3 models to evaluate: multinomial logistic regression, random forest, and xgboost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultinom_mod <-\n  multinom_reg() %>%\n  set_engine(\"nnet\") %>% \n  set_mode(\"classification\")\n\nranger_mod <-\n  rand_forest() %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n\nxgboost_mod <-\n  boost_tree() %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n## Model fit function\n\nWith 3 models and 5 categorical encodings, I'll need to fit 15 models. To streamline this process, I'll define two functions:\n\n- `fit_model()`: Given training and test datasets, a workflow containing a recipe for the categorical encoding, a model type, and an encoding type, this function will evaluate the model in-sample using cross-validation, then evaluate it out-of-sample, and then return a dataframe containing the results\n- `fit_encodings()`: Given a model and model type, this function will generate recipes for each of the 5 categorical encodings, fit the 5 encodings using the model, and then return a dataframe with the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_model <- function(train, test, workflow, model_type, encoding_type){\n  \n  set.seed(42)\n  folds <- vfold_cv(train, v = 5)\n  \n  resampled_fit <- \n    workflow %>% \n    fit_resamples(folds,\n                  metrics = metric_set(f_meas))\n  \n  # Get in-sample F1\n  (resampled_fit %>%\n    collect_metrics())$mean -> train_perf\n  \n  # Get out-of-sample F1\n  fit <- \n    workflow %>%\n    fit(train)\n  \n  test$pred <- predict(fit, test)$.pred_class\n  (f_meas(test, outcome, pred, estimator='micro'))$.estimate -> test_perf\n  \n  # Combine in-sample and out-of-sample into a dataframe\n  df_perf <- data.frame(model_type = model_type,\n                        encoding_type = encoding_type,\n                        train_perf = train_perf,\n                        test_perf = test_perf)\n  return(df_perf)\n}\n\n\n# Given a model, run it across the 4 encodings and return a dataframe that summarizes the results\nfit_encodings <- function(model, model_type){\n  \n  set.seed(42)\n  tensorflow::set_random_seed(42)\n  \n  # One-hot encoded model\n  wflow_1hot <- \n    workflow() %>% \n    add_model(model) %>%\n    add_recipe(recipe_1hot_with_novel)\n  \n  fit_model(train %>% select(-id), \n            test %>% select(-id), \n            wflow_1hot, \n            model_type,\n            'onehot') -> onehot_model_results\n  \n  # Label encoded model\n  wflow_label <- \n    workflow() %>% \n    add_model(model) %>%\n    add_recipe(recipe_label)\n  \n  fit_model(train %>% select(-id), \n            test %>% select(-id), \n            wflow_label, \n            model_type,\n            'label') -> label_model_results\n  \n  # Frequency encoded model\n  wflow_freq <- \n    workflow() %>% \n    add_model(model) %>%\n    add_recipe(recipe_freq)\n  \n  fit_model(train_freq %>% select(-id), \n            test_freq %>% select(-id), \n            wflow_freq, \n            model_type,\n            'frequency') -> freq_model_results\n  \n  # Target encoded model\n  wflow_target <- \n    workflow() %>% \n    add_model(model) %>%\n    add_recipe(recipe_target)\n  \n  fit_model(train_te %>% as.data.frame() %>% select(-id), \n            test_te %>% as.data.frame() %>% select(-id), \n            wflow_target, \n            model_type,\n            'target') -> target_model_results\n  \n  \n  # Embedding encoded model\n  wflow_embedding <- \n    workflow() %>% \n    add_model(model) %>%\n    add_recipe(recipe_embedding)\n  \n  fit_model(train %>% as.data.frame() %>% select(-id), \n            test %>% as.data.frame() %>% select(-id), \n            wflow_embedding, \n            model_type,\n            'onehot + embedding') -> embedding_model_results\n  \n  # Compile results into a dataframe\n  onehot_model_results %>%\n    bind_rows(label_model_results) %>%\n    bind_rows(freq_model_results) %>%\n    bind_rows(target_model_results) %>%\n    bind_rows(embedding_model_results) -> results\n  \n  results\n}\n```\n:::\n\n\nI'll run each of the models using the `fit_encodings()` and `fit_model()` functions that I just earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_encodings(multinom_mod, 'multinomial logistic') -> multinom_results\nfit_encodings(ranger_mod, 'random forest') -> rf_results\nfit_encodings(xgboost_mod, 'xgboost') -> xgb_results\n```\n:::\n\n\n# Model results\n\nAnd finally I can summarize the results. \n\nFrom this summary, I can see that among the categorical encoding methods target performed best on average, frequency performed worst on average, but the best models used the one-hot or one-hot + embedding encoding strategies. And in terms of ML algorithm, the two tree-based models performed better, on average, as compared to the logistic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultinom_results %>%\n  bind_rows(rf_results) %>%\n  bind_rows(xgb_results) -> model_results\n\nmodel_results %>%\n  arrange(desc(test_perf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             model_type      encoding_type train_perf test_perf\n1         random forest onehot + embedding  0.6919299 0.7119741\n2         random forest             onehot  0.6924099 0.7087379\n3               xgboost             target  0.6809957 0.7003891\n4         random forest             target  0.6823033 0.6964981\n5         random forest              label  0.6823488 0.6925566\n6  multinomial logistic             target  0.6425861 0.6887160\n7               xgboost              label  0.6883483 0.6860841\n8         random forest          frequency  0.6741463 0.6828479\n9               xgboost             onehot  0.6765134 0.6828479\n10              xgboost          frequency  0.6681338 0.6828479\n11              xgboost onehot + embedding  0.6740048 0.6796117\n12 multinomial logistic          frequency  0.6326412 0.6731392\n13 multinomial logistic             onehot  0.6462751 0.6699029\n14 multinomial logistic onehot + embedding  0.6336241 0.6634304\n15 multinomial logistic              label  0.6268817 0.6537217\n```\n:::\n\n```{.r .cell-code}\nmodel_results %>%\n  ggplot() +\n    geom_col(aes(x = encoding_type, group = model_type, fill = model_type, y = test_perf), position='dodge') +\n    scale_y_continuous(limits=c(0.6,0.75),oob = rescale_none)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_results %>%\n  ggplot() +\n    geom_col(aes(x = model_type, group = encoding_type, fill = encoding_type, y = test_perf), position='dodge') +\n    scale_y_continuous(limits=c(0.6,0.75),oob = rescale_none)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}