{
  "hash": "d665b2c9004028c63cfadec33c2eb503",
  "result": {
    "markdown": "---\ntitle: 'Building a prediction model to detect spam email'\ndate: 2023-08-19\ndescription: \"Using the spam email dataset from Tidy Tuesday Week 33, I walk through the process of building and evaluating a prediction model using decision tree and random forest machine learning algorithms.\"\nimage: social-image.png\ntwitter-card:\n  image: \"social-image.png\"\nopen-graph:\n  image: \"social-image.png\"\ncategories:\n  - tidy-tuesday\n  - statistical-modeling\n  - machine-learning\n  - prediction\n---\n\n\nGetting back into the swing of things. This is my first blog post in more than 3 years!\n\nFor this post, I'll be using the Week 33 [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday) dataset. This one is all about spam email. \n\nFrom the <a href=\"https://vincentarelbundock.github.io/Rdatasets/doc/DAAG/spam7.html\">dataset description</a>:\n\n> The data consist of 4601 email items, of which 1813 items were identified as spam. This is a subset of the full dataset, with six only of the 57 explanatory variables in the complete dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(tidymodels)\nlibrary(usemodels)\nlibrary(future)\nlibrary(rpart)\nlibrary(rpart.plot)\nknitr::opts_chunk$set(echo = TRUE, fig.width = 4.5, fig.height = 2.5)\n```\n:::\n\n\n# Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv') %>%\n  mutate(yesno = factor(yesno == 'y', levels=c(TRUE, FALSE)))\n```\n:::\n\n\n# Data dictionary\n\n```\nvariable    class         description\ncrl.tot     double        Total length of uninterrupted sequences of capitals\ndollar      double        Occurrences of the dollar sign, as percent of total number of characters\nbang        double        Occurrences of ‘!’, as percent of total number of characters\nmoney       double        Occurrences of ‘money’, as percent of total number of characters\nn000        double        Occurrences of the string ‘000’, as percent of total number of words\nmake        double        Occurrences of ‘make’, as a percent of total number of words\nyesno       character     Outcome variable, a factor with levels 'n' not spam, 'y' spam\n```\n\nThe outcome variable is `yesno`, a character type, and all of the other variables are of a numeric type.\n\nWe can see that a lot of the feature engineering has already been done. It seems that whoever prepared this dataset has determined that spam emails can be meaningfully identified by:\n\n- SHOUTING! (`crl.tot`, `bang`)\n- and talk of making money (`make`, `money`, `dollar`) at values of 1,000 or greater (`n000`)\n\nThat definitely matches my experience reading spam!\n\n\n# What can we do with this data?\n\nWhat can we do with this data? Because this data has \"ground truth\" labels that tell us which emails were spam or not (`yesno`), and a list of features associated with each email, we can approach this as a supervised ML problem. We can use supervised ML to predict spam, to create a spam filter, for example.\n\n# Supervised ML: Predicting spam / spam filter\n\n## Average values, by spam\n\nLet's start by looking at some averages (mean and median), split by the outcome variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  group_by(yesno) %>%\n  summarise_all(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  yesno crl.tot dollar  bang  money    n000   make\n  <fct>   <dbl>  <dbl> <dbl>  <dbl>   <dbl>  <dbl>\n1 TRUE     471. 0.174  0.514 0.213  0.247   0.152 \n2 FALSE    161. 0.0116 0.110 0.0171 0.00709 0.0735\n```\n:::\n:::\n\n\nWe can see that on average, spam emails have higher mean values for each of the predictors. No surprise there.\n\nHowever, the medians of some variables are zero, which suggests those variables have heavily positively skewed distributions with many zero values (sometimes called \"zero-inflation\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  group_by(yesno) %>%\n  summarise_all(median)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  yesno crl.tot dollar  bang money  n000  make\n  <fct>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 TRUE      194   0.08 0.331     0     0     0\n2 FALSE      54   0    0         0     0     0\n```\n:::\n:::\n\n\nWe can confirm this by looking at the counts of zero values, in relation to the total counts.\n\nAs we see, the vast majority of the spam emails had non-zero values on these variables, and non-spam emails had significantly fewer non-zero values, with the exception of `crl.tot`. In particular, spam emails were MUCH more likely to contain \"!\", \"$\", \"000\", and \"money\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno = df %>% filter(yesno == FALSE) %>% select(-yesno)\nyes = df %>% filter(yesno == TRUE) %>% select(-yesno)\n\nround(colSums(no>0)/nrow(no)*100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncrl.tot  dollar    bang   money    n000    make \n    100      10      27       2       3      15 \n```\n:::\n\n```{.r .cell-code}\nround(colSums(yes>0)/nrow(yes)*100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncrl.tot  dollar    bang   money    n000    make \n    100      61      83      38      33      35 \n```\n:::\n:::\n\n\n## Distributions, by spam\n\nNext, let's look at the distributions.\n\nFor these plots, since they all have extreme skew, I'm going to truncate them at the 90th percentile and look at the left side where most of the mass is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (c in c('crl.tot', 'dollar', 'bang', 'money', 'n000', 'make')){\n  \n  qtile_90 <- quantile(df[[c]], .90)\n  \n  df %>%\n    filter(!!sym(c) < qtile_90) %>%\n    ggplot(aes(x = !!sym(c), fill=yesno)) +\n      geom_density(alpha=.7) +\n      ggtitle(c) -> plot\n  print(plot)\n  \n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-4.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-5.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-6.png){width=432}\n:::\n:::\n\n\n## Feature correlations\n\nIt doesn't look like the features are very strongly correlated. The strongest correlation is between `n000` and `dollar`, which is not particularly surprising since I would expect that \"000\" would tend to appear in the context of a dollar value like \"$1000\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% \n  select(-yesno) %>%\n  cor(use = \"complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           crl.tot    dollar       bang      money       n000       make\ncrl.tot 1.00000000 0.2019477 0.03632120 0.08099318 0.16597657 0.08916478\ndollar  0.20194768 1.0000000 0.14291296 0.10469131 0.31097072 0.11741853\nbang    0.03632120 0.1429130 1.00000000 0.05107591 0.07010334 0.05829200\nmoney   0.08099318 0.1046913 0.05107591 1.00000000 0.05258693 0.18815518\nn000    0.16597657 0.3109707 0.07010334 0.05258693 1.00000000 0.13407211\nmake    0.08916478 0.1174185 0.05829200 0.18815518 0.13407211 1.00000000\n```\n:::\n:::\n\nIf we convert the features to boolean, we can see that the presence of features have stronger correlations. The strongest correlation is again between `dollar` and `n000`, but `money` and `dollar` also occur together more often than not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  select(-yesno, -crl.tot) %>%\n  mutate(dollar = dollar > 0,\n         bang = bang > 0,\n         money = money > 0,\n         n000 = n000 > 0,\n         make = make > 0\n  ) %>%\n  cor(use = \"complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          dollar      bang     money      n000      make\ndollar 1.0000000 0.3779057 0.5058803 0.5372603 0.4133374\nbang   0.3779057 1.0000000 0.3290505 0.3404896 0.2641339\nmoney  0.5058803 0.3290505 1.0000000 0.4140177 0.4092119\nn000   0.5372603 0.3404896 0.4140177 1.0000000 0.3757565\nmake   0.4133374 0.2641339 0.4092119 0.3757565 1.0000000\n```\n:::\n:::\n\n\n## Simple classification algorithm\n\nJust for fun, let's see how well we can distinguish spam vs. not spam using a simple heuristic. \n\nI'll label anything as spam if it contained at least 1 \"money\", \"$\", \"000\", and \"!\" OR if it contained more than 100 uninterrupted sequences of capital letters and at least 1 \"!\". This is just what comes to mind after looking at the frequency plots above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  mutate(simple_spam_flag = factor((money > 0 & dollar > 0 & bang > 0 & n000 > 0) | \n                                     (crl.tot > 100 & bang > 0), \n                                   levels=c(TRUE, FALSE))\n         ) -> df_flag\n```\n:::\n\n\nThis simple classification algorithm achieved an accuracy of 79%, with 64% sensitivity and 89% specificity. This doesn't seem too bad. But what is a good baseline of performance?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(df_flag$simple_spam_flag, df_flag$yesno, mode='everything')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE  1172   309\n     FALSE  641  2479\n                                          \n               Accuracy : 0.7935          \n                 95% CI : (0.7815, 0.8051)\n    No Information Rate : 0.606           \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.5533          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.6464          \n            Specificity : 0.8892          \n         Pos Pred Value : 0.7914          \n         Neg Pred Value : 0.7946          \n              Precision : 0.7914          \n                 Recall : 0.6464          \n                     F1 : 0.7116          \n             Prevalence : 0.3940          \n         Detection Rate : 0.2547          \n   Detection Prevalence : 0.3219          \n      Balanced Accuracy : 0.7678          \n                                          \n       'Positive' Class : TRUE            \n                                          \n```\n:::\n:::\n\nWe can see that the base rate of spam is 39%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(df$yesno == TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3940448\n```\n:::\n:::\n\n\nA good baseline model might be to predict the majority class, which in this case is not-spam.\n\nThis \"always predict FALSE\" baseline model can be expected to achieve 1 minus the base rate of spam (i.e., 61%), and we can see that this is the case if we construct just such a model. This tells us that the heuristic model above is quite a bit better than a completely naive model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(factor(rep('FALSE',nrow(df_flag))), df_flag$yesno, mode='everything')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in confusionMatrix.default(factor(rep(\"FALSE\", nrow(df_flag))), :\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE     0     0\n     FALSE 1813  2788\n                                          \n               Accuracy : 0.606           \n                 95% CI : (0.5917, 0.6201)\n    No Information Rate : 0.606           \n    P-Value [Acc > NIR] : 0.5064          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.000           \n            Specificity : 1.000           \n         Pos Pred Value :   NaN           \n         Neg Pred Value : 0.606           \n              Precision :    NA           \n                 Recall : 0.000           \n                     F1 :    NA           \n             Prevalence : 0.394           \n         Detection Rate : 0.000           \n   Detection Prevalence : 0.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : TRUE            \n                                          \n```\n:::\n:::\n\n\n\n\n## Decision Tree\n\nProceeding from simpler to more complex, we can go a step further and try fitting a decision tree model. The decision tree will help us to identify a more sophisticated rule set for classifying spam mail. Decision trees also have the advantage of being highly interpretable.\n\nWe'll start by splitting our data into training and test -- that way, we won't be testing performance on the same data that our model was trained on, and we can minimize the risk of overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(200)\nsplit <- initial_split(df)\ntrain <- training(split)\ntest <- testing(split)\n```\n:::\n\n\nNext, we'll fit the model and then visualize its logic.\n\nWe can read this chart by starting at the root node and following the branches until we reach a terminal node. The predicted value at this terminal node will give us the prediction that the model has made, and the path that we followed to get there provides its reasoning for the prediction.\n\nSo for example, if we follow the tree to the left-most terminal node, we can see that it would predict that an email was spam if it contained `dollar >= 0.056`. If we follow the tree to the right-most terminal, we can see that it would predict that an email was not spam if it contained `dollar < 0.056` and `bang >= 0.12`. The percentage value in the node tells us what percentage of emails met these criteria. So 24% of emails met the former criteria, and 54% the latter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecision_tree <- rpart(yesno ~ ., data=train, method='class')\ndecision_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 3450 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 3450 1358 FALSE (0.3936232 0.6063768)  \n   2) dollar>=0.0555 839   94 TRUE (0.8879619 0.1120381) *\n   3) dollar< 0.0555 2611  613 FALSE (0.2347759 0.7652241)  \n     6) bang>=0.1205 739  330 TRUE (0.5534506 0.4465494)  \n      12) crl.tot>=80.5 361   75 TRUE (0.7922438 0.2077562) *\n      13) crl.tot< 80.5 378  123 FALSE (0.3253968 0.6746032)  \n        26) bang>=0.802 85   34 TRUE (0.6000000 0.4000000) *\n        27) bang< 0.802 293   72 FALSE (0.2457338 0.7542662) *\n     7) bang< 0.1205 1872  204 FALSE (0.1089744 0.8910256) *\n```\n:::\n\n```{.r .cell-code}\nrpart.plot(decision_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=432}\n:::\n:::\n\n\nFinally, we can test the model on the out-of-sample test dataset and see how it performs.\n\nOverall it achieved an accuracy of 85%, with 78% sensitivity and 89% specificity. This model has similar specificity as the heuristic model, but better sensitivity, and therefore better overall accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred <- predict(decision_tree, test, type='class')\nconfusionMatrix(test_pred, test$yesno, mode='everything')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   355    77\n     FALSE  100   619\n                                          \n               Accuracy : 0.8462          \n                 95% CI : (0.8241, 0.8666)\n    No Information Rate : 0.6047          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.6755          \n                                          \n Mcnemar's Test P-Value : 0.0982          \n                                          \n            Sensitivity : 0.7802          \n            Specificity : 0.8894          \n         Pos Pred Value : 0.8218          \n         Neg Pred Value : 0.8609          \n              Precision : 0.8218          \n                 Recall : 0.7802          \n                     F1 : 0.8005          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3084          \n   Detection Prevalence : 0.3753          \n      Balanced Accuracy : 0.8348          \n                                          \n       'Positive' Class : TRUE            \n                                          \n```\n:::\n:::\n\n\n## Random forest\n\nNext, we'll try a random forest model. Random forest models tend to perform better than decision trees, due to the fact that they are ensemble decision trees, meaning they group together the decisions of lots of decision trees. But as a result, they tend to be less interpretable. So if our goal was only to create the most accurate prediction model possible, then a random forest would be better suited to the task.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv <- vfold_cv(train)\n```\n:::\n\n\nI'll use `usemodels::use_ranger` to give me a starting template.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_ranger(yesno ~ ., train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nranger_recipe <- \n  recipe(formula = yesno ~ ., data = train) \n\nranger_spec <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") \n\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec) \n\nset.seed(17214)\nranger_tune <-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n```\n:::\n:::\n\n\nI'll remove the parameter tuning to keep things simple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_recipe <- \n  recipe(formula = yesno ~ ., data = df)\n\nranger_spec <- \n  rand_forest(trees = 1000) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") \n\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec) \n```\n:::\n\n\nNext, I'll fit the model using a resampling approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nplan(multisession)\n\nfit_rf <- fit_resamples(\n  ranger_workflow,\n  cv,\n  metrics = metric_set(accuracy, sens, spec),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE,\n                              extract = function(x) x)\n)\n```\n:::\n\n\nOverall, accuracy is pretty good: 89% accuracy, 79% sensitivity, and 95% specificity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_rf %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.886    10 0.00497 Preprocessor1_Model1\n2 sens     binary     0.793    10 0.00797 Preprocessor1_Model1\n3 spec     binary     0.946    10 0.00523 Preprocessor1_Model1\n```\n:::\n:::\n\nNext, we can check the performance on the test set.\n\nWe can use `collect_metrics()` function on the last fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_workflow %>%\n  last_fit(split) %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.888 Preprocessor1_Model1\n2 roc_auc  binary         0.930 Preprocessor1_Model1\n```\n:::\n:::\n\n\nOr we can use `confusionMatrix()` to get a bit more information.\n\nPerformance on the test set is similar to the training performance. Overall, accuracy is pretty good -- and better than the decision tree. For a spam detection filter, we'd want to bias towards minimizing false positives (it would arguably be worse for people to lose legitimate mail to the filter, than to have spam mail slip through), and here we see that the specificity was quite good at ~95%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_workflow %>%\n  last_fit(split) %>% \n  extract_workflow() -> final_model\n\nconfusionMatrix(predict(final_model, test)$.pred_class, test$yesno, mode='everything', positive='TRUE')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   362    34\n     FALSE   93   662\n                                          \n               Accuracy : 0.8897          \n                 95% CI : (0.8701, 0.9072)\n    No Information Rate : 0.6047          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.7639          \n                                          \n Mcnemar's Test P-Value : 2.652e-07       \n                                          \n            Sensitivity : 0.7956          \n            Specificity : 0.9511          \n         Pos Pred Value : 0.9141          \n         Neg Pred Value : 0.8768          \n              Precision : 0.9141          \n                 Recall : 0.7956          \n                     F1 : 0.8508          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3145          \n   Detection Prevalence : 0.3440          \n      Balanced Accuracy : 0.8734          \n                                          \n       'Positive' Class : TRUE            \n                                          \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}